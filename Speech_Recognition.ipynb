{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Speech Recognition.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "8KrrzNGd8KCX",
        "G7s_-PkZvFLS",
        "TNCjxtUT8Pcf",
        "CUDbmmBX8Vih",
        "tD2s1mpvD5f8",
        "xk_fdc7PkyoN",
        "UMcY4Jtg7tt2"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOu6UUlvJb+U6Yu571+olJi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/renardelyon/Pronunciation-Learning-with-Translator/blob/main/Speech_Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCI8UwvjIP_J"
      },
      "source": [
        "!pip install pydub\n",
        "!pip install tensorflow-io==0.17\n",
        "!pip install mutagen"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJRSZDQuBLS5"
      },
      "source": [
        "import os\n",
        "import pathlib\n",
        "import re\n",
        "import shutil\n",
        "import mutagen\n",
        "import math\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_io as tfio\n",
        "\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "from tensorflow import keras\n",
        "from IPython import display\n",
        "from pydub import AudioSegment\n",
        "from mutagen.wave import WAVE\n",
        "from string import ascii_lowercase"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KrrzNGd8KCX"
      },
      "source": [
        "### GetCleanFile Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewBM_6fcbFuq"
      },
      "source": [
        "class GetCleanFile:\n",
        "  def __init__(self, origin, new_path, newer_path, zip_file):\n",
        "    self.origin = origin\n",
        "    self.new_path = new_path\n",
        "    self.newer_path = newer_path\n",
        "    self.zip_file = zip_file\n",
        "    self.train_path = \"\"\n",
        "    \n",
        "    # make new directory to contain organized sub-directory\n",
        "    if not os.path.exists(self.newer_path):\n",
        "      os.mkdir(self.newer_path)\n",
        "  \n",
        "  def __call__(self):\n",
        "    # download data from the web server\n",
        "    data_dir = pathlib.Path(self.new_path)\n",
        "    \n",
        "    if not data_dir.exists():\n",
        "      tf.keras.utils.get_file(\n",
        "          self.zip_file,\n",
        "          origin = self.origin,\n",
        "          extract = True,\n",
        "          cache_dir = '.',\n",
        "          cache_subdir = self.new_path.split('/')[-1])\n",
        "      \n",
        "    return self\n",
        "  \n",
        "  def get_path (self):\n",
        "    '''get data directory path'''\n",
        "    all_file = os.listdir(self.new_path)\n",
        "    dir = [i for i in all_file if not re.match('[\\w]*.zip',i)][0]\n",
        "    path = os.path.join(self.new_path,dir)\n",
        "    train_test_dir = [i for i in os.listdir(path) if not re.match('[\\w]*.TXT',i)][0]\n",
        "    self.train_path = os.path.join(path,train_test_dir)\n",
        "    return self\n",
        "\n",
        "  def get_subdirectory(self):\n",
        "    '''including subdirectories and excluding upper directories'''\n",
        "    return tf.io.gfile.glob(str(self.train_path)+'/*/*')\n",
        "\n",
        "  def rename_and_move_dir(self, dir_names):\n",
        "    ''' rename the sub-directory and move the subdirectory\n",
        "        to another directory'''\n",
        "    for i, dir in enumerate(dir_names):\n",
        "        split_dir = dir.split('/')\n",
        "        split_dir[-1] = str(i)\n",
        "        joined_dir = '/'.join(split_dir)\n",
        "        shutil.move(dir, joined_dir)\n",
        "        shutil.move(joined_dir, self.newer_path)\n",
        "    return self\n",
        "  \n",
        "  def delete_directory(self):\n",
        "    '''delete initial data directory'''\n",
        "    shutil.rmtree(self.new_path)\n",
        "\n",
        "  def clean_label(self, subdirs):\n",
        "    '''process the label so its content does not have filename in front of each\n",
        "        lines'''\n",
        "    for subdir in subdirs:\n",
        "      # Define sub-directory for the new files\n",
        "      new_subdir = subdir.split('/')[:-1]\n",
        "      new_subdir = '/'.join(new_subdir)\n",
        "\n",
        "      with open(subdir, 'r') as f:\n",
        "      \n",
        "        # Read all lines and return as list\n",
        "        lines = f.readlines()\n",
        "\n",
        "        # iterate line by line\n",
        "        for line in lines:\n",
        "          new_name = line.split()[0]\n",
        "          content = ' '.join(line.split()[1:]).lower()\n",
        "          file_subdir = os.path.join(new_subdir, f'{new_name}.txt')\n",
        "          with open(file_subdir, 'w') as new_file:\n",
        "            new_file.write(content)\n",
        "\n",
        "        # delete initial text file\n",
        "        os.remove(subdir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7s_-PkZvFLS"
      },
      "source": [
        "### EncodingDecoding Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sb6DOUzMvmj7"
      },
      "source": [
        "class EncodingDecoding:\n",
        "  def __init__(self):\n",
        "    self.char = [c for c in ascii_lowercase]\n",
        "    self.non_alpha = [\"-\",\" \", \"'\"]\n",
        "    self.non_alpha.extend(self.char)  \n",
        "\n",
        "  def encode_label(self, label):\n",
        "    keys_tensor = tf.constant(self.non_alpha)\n",
        "    vals_tensor = tf.constant(np.arange(len(self.non_alpha)))\n",
        "    input_tensor = label\n",
        "\n",
        "    table = tf.lookup.StaticHashTable(\n",
        "        tf.lookup.KeyValueTensorInitializer(keys_tensor, vals_tensor),\n",
        "        default_value=-1)\n",
        "    \n",
        "    return table.lookup(input_tensor)\n",
        "\n",
        "  def decode_label(self,predicted_label):\n",
        "    keys_tensor = tf.constant(np.arange(len(self.non_alpha)))\n",
        "    vals_tensor = tf.constant(self.non_alpha)\n",
        "    input_tensor = predicted_label\n",
        "\n",
        "    table = tf.lookup.StaticHashTable(\n",
        "        tf.lookup.KeyValueTensorInitializer(keys_tensor, vals_tensor),\n",
        "        default_value='')\n",
        "    \n",
        "    return table.lookup(input_tensor).numpy()\n",
        "  \n",
        "  def decode_audio(self, audio_binary):\n",
        "    ''' decode wav file to float tensor'''\n",
        "    waveform, _ = tf.audio.decode_wav(audio_binary)\n",
        "    return tf.squeeze(waveform,axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNCjxtUT8Pcf"
      },
      "source": [
        "### AudioFileConversion Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fua0IT4c1IeN"
      },
      "source": [
        "class AudioFileConversion:\n",
        "  def convert_flac_to_wav(self, src, dst):\n",
        "    flac_audio = AudioSegment.from_file(src,format=\"flac\")\n",
        "    flac_audio.export(dst, format=\"wav\") \n",
        "\n",
        "  def file_conversion(self, path):\n",
        "    '''convert flac file into wav file'''\n",
        "    for i, (subdirs, dir, fnames) in enumerate(os.walk(path)):\n",
        "      if i > 0: \n",
        "        fnames = [fname for fname in fnames if not re.match('[\\w\\d.-]*.txt',fname)]\n",
        "        for fname in fnames:\n",
        "\n",
        "          # creating source path and destination path for the converted file\n",
        "          src = os.path.join(subdirs,fname)\n",
        "          fname_split = fname.split('.')\n",
        "          fname_split[-1]='wav'\n",
        "          fname = '.'.join(fname_split)\n",
        "          dst =  os.path.join(subdirs, fname)\n",
        "\n",
        "          # convert flac file format into wav file format\n",
        "          self.convert_flac_to_wav(src, dst)\n",
        "\n",
        "          # delete initial flac file\n",
        "          os.remove(src) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUDbmmBX8Vih"
      },
      "source": [
        "### AudioDataProcessing Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dE6KSIBkfy3Y"
      },
      "source": [
        "class AudioDataProcessing:\n",
        "  def __init__(self):\n",
        "    self.max_length = 25\n",
        "    self.sample_rate = 16000\n",
        "    \n",
        "  def get_spectrogram(self, waveform):\n",
        "    '''Create spectogram from audio wave form'''\n",
        "    # Padding for files with less than max sample\n",
        "    max_sample = int(self.max_length * self.sample_rate)\n",
        "    zero_padding = tf.zeros([max_sample] - tf.shape(waveform), dtype=tf.float32)\n",
        "\n",
        "    # Concatenate audio with padding so that all audio clips will be of the \n",
        "    # same length\n",
        "    waveform = tf.cast(waveform, tf.float32)\n",
        "    equal_length = tf.concat([waveform, zero_padding], 0)\n",
        "    spectrogram = tf.signal.stft(\n",
        "        equal_length, frame_length=1024, \n",
        "        frame_step = 892)\n",
        "      \n",
        "    spectrogram = tf.abs(spectrogram)\n",
        "\n",
        "    return spectrogram\n",
        "\n",
        "    #Spoken Word Recognition Using MFCC and Learning Vector Quantization\n",
        "  def get_log_mel_spectrograms(self, spectrogram):\n",
        "    '''extract log mel spectrogram from spectrogram'''\n",
        "    num_spectrogram_bins = spectrogram.shape[-1]\n",
        "    num_mel_bins, lower_edge_hertz, upper_edge_hertz = 13, 250, 8000\n",
        "    weight = tf.signal.linear_to_mel_weight_matrix(num_mel_bins, num_spectrogram_bins,\n",
        "                                                 self.sample_rate, lower_edge_hertz,\n",
        "                                                 upper_edge_hertz)\n",
        "    mel_spectrograms = tf.tensordot(spectrogram,weight,1)\n",
        "    mel_spectrograms.set_shape(spectrogram.shape[:-1].concatenate(\n",
        "            weight.shape[-1:]))\n",
        "  \n",
        "    log_mel_spectrograms = tf.math.log(mel_spectrograms + 1e-6)\n",
        "    return log_mel_spectrograms\n",
        "  \n",
        "  def get_mfcc(self, log_mel_spectrograms):\n",
        "    '''extract mel frequency ceptrums coefficients from audio waveform'''\n",
        "    mfcc = tf.signal.mfccs_from_log_mel_spectrograms(log_mel_spectrograms)\n",
        "    return mfcc\n",
        "\n",
        "  def spec_augment(self):\n",
        "    '''perform data augmentation for audio log spectrogram'''\n",
        "    param = np.random.randint(1,100)\n",
        "    augmentation = tf.keras.Sequential([\n",
        "       layers.Lambda(lambda x : tfio.experimental.audio.freq_mask(x, param)),\n",
        "       layers.Lambda(lambda x : tfio.experimental.audio.time_mask(x, param))            \n",
        "    ])\n",
        "\n",
        "    return augmentation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tD2s1mpvD5f8"
      },
      "source": [
        "### GetWaveformLabel Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6sksXMP-M2v"
      },
      "source": [
        "class GetWaveformLabel(EncodingDecoding):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.max_length = 400\n",
        "    \n",
        "  def get_waveform_label(self, audio_file, text_file):\n",
        "    # decode WAV audio file\n",
        "    audio_data = tf.io.read_file(audio_file)\n",
        "    waveform = super().decode_audio(audio_data)\n",
        "    \n",
        "    #convert tensor into str\n",
        "    text = tf.io.read_file(text_file)\n",
        "\n",
        "    #split char from whole string\n",
        "    chars = tf.strings.bytes_split(text)\n",
        "\n",
        "    # encode text file to numeric values  \n",
        "    label = super().encode_label(chars)\n",
        "\n",
        "    zero_padding = tf.zeros([self.max_length] - tf.shape(label), dtype=tf.int64)\n",
        "\n",
        "    # Concatenate encode text with padding so that all encode text will be of the \n",
        "    # same length\n",
        "    label = tf.concat([label, zero_padding], 0)\n",
        "\n",
        "    return waveform, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xk_fdc7PkyoN"
      },
      "source": [
        "### GetProcessDataLabel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xt2ggNcgjIFO"
      },
      "source": [
        "class GetProcessDataLabel(AudioDataProcessing):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "  \n",
        "  def get_process_label(self, waveform, label, train):\n",
        "    x = super().get_spectrogram(waveform)\n",
        "    if train:\n",
        "      x = super().spec_augment()(x)\n",
        "    x = super().get_log_mel_spectrograms(x)\n",
        "    x = super().get_mfcc(x)\n",
        "    return x, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMcY4Jtg7tt2"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DBlO4L5Q1H-"
      },
      "source": [
        "class TokenEmbedding(layers.Layer):\n",
        "    def __init__(self, num_vocab=1000, maxlen=100, num_hid=64):\n",
        "        super().__init__()\n",
        "        self.emb = tf.keras.layers.Embedding(num_vocab, num_hid)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=num_hid)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        x = self.emb(x)\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        return x + positions\n",
        "\n",
        "\n",
        "class SpeechFeatureEmbedding(layers.Layer):\n",
        "    def __init__(self, num_hid=64, maxlen=100):\n",
        "        super().__init__()\n",
        "        self.conv1 = tf.keras.layers.Conv1D(\n",
        "            num_hid, 11, strides=2, padding=\"same\", activation=\"relu\"\n",
        "        )\n",
        "        self.conv2 = tf.keras.layers.Conv1D(\n",
        "            num_hid, 11, strides=2, padding=\"same\", activation=\"relu\"\n",
        "        )\n",
        "        self.conv3 = tf.keras.layers.Conv1D(\n",
        "            num_hid, 11, strides=2, padding=\"same\", activation=\"relu\"\n",
        "        )\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=num_hid)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        return self.conv3(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVZCcl_Gzdbt"
      },
      "source": [
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, feed_forward_dim, rate=0.1):\n",
        "        super().__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(feed_forward_dim, activation=\"relu\"),\n",
        "                layers.Dense(embed_dim),\n",
        "            ]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output) #residual\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output) #residual"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HALtJDoi1H_K"
      },
      "source": [
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, feed_forward_dim, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.self_att = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.enc_att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.self_dropout = layers.Dropout(0.5)\n",
        "        self.enc_dropout = layers.Dropout(0.1)\n",
        "        self.ffn_dropout = layers.Dropout(0.1)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(feed_forward_dim, activation=\"relu\"),\n",
        "                layers.Dense(embed_dim),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def causal_attention_mask(self, batch_size, n_dest, n_src, dtype):\n",
        "        \"\"\"Masks the upper half of the dot product matrix in self attention.\n",
        "\n",
        "        This prevents flow of information from future tokens to current token.\n",
        "        1's in the lower triangle, counting from the lower right corner.\n",
        "        \"\"\"\n",
        "        i = tf.range(n_dest)[:, None]\n",
        "        j = tf.range(n_src)\n",
        "        m = i >= j - n_src + n_dest\n",
        "        mask = tf.cast(m, dtype)\n",
        "        mask = tf.reshape(mask, [1, n_dest, n_src])\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n",
        "        )\n",
        "        return tf.tile(mask, mult)\n",
        "\n",
        "    def call(self, enc_out, target):\n",
        "        input_shape = tf.shape(target)\n",
        "        batch_size = input_shape[0]\n",
        "        seq_len = input_shape[1]\n",
        "        causal_mask = self.causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n",
        "        target_att = self.self_att(target, target, attention_mask=causal_mask)\n",
        "        target_norm = self.layernorm1(target + self.self_dropout(target_att))\n",
        "        enc_out = self.enc_att(target_norm, enc_out)\n",
        "        enc_out_norm = self.layernorm2(self.enc_dropout(enc_out) + target_norm)\n",
        "        ffn_out = self.ffn(enc_out_norm)\n",
        "        ffn_out_norm = self.layernorm3(enc_out_norm + self.ffn_dropout(ffn_out))\n",
        "        return ffn_out_norm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgR0GA6sFd_c"
      },
      "source": [
        "class Transformer(keras.Model):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_hid=64,\n",
        "        num_head=2,\n",
        "        num_feed_forward=128,\n",
        "        source_maxlen=100,\n",
        "        target_maxlen=100,\n",
        "        num_layers_enc=4,\n",
        "        num_layers_dec=1,\n",
        "        num_classes=10,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.loss_metric = keras.metrics.Mean(name=\"loss\")\n",
        "        self.num_layers_enc = num_layers_enc\n",
        "        self.num_layers_dec = num_layers_dec\n",
        "        self.target_maxlen = target_maxlen\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        self.enc_input = SpeechFeatureEmbedding(num_hid=num_hid, maxlen=source_maxlen)\n",
        "        self.dec_input = TokenEmbedding(\n",
        "            num_vocab=num_classes, maxlen=target_maxlen, num_hid=num_hid\n",
        "        )\n",
        "\n",
        "        self.encoder = keras.Sequential(\n",
        "            [self.enc_input]\n",
        "            + [\n",
        "                TransformerEncoder(num_hid, num_head, num_feed_forward)\n",
        "                for _ in range(num_layers_enc)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        for i in range(num_layers_dec):\n",
        "            setattr(\n",
        "                self,\n",
        "                f\"dec_layer_{i}\",\n",
        "                TransformerDecoder(num_hid, num_head, num_feed_forward),\n",
        "            )\n",
        "\n",
        "        self.classifier = layers.Dense(num_classes)\n",
        "\n",
        "    def decode(self, enc_out, target):\n",
        "        y = self.dec_input(target)\n",
        "        for i in range(self.num_layers_dec):\n",
        "            y = getattr(self, f\"dec_layer_{i}\")(enc_out, y)\n",
        "        return y\n",
        "\n",
        "    def call(self, inputs):\n",
        "        source = inputs[0]\n",
        "        target = inputs[1]\n",
        "        x = self.encoder(source)\n",
        "        y = self.decode(x, target)\n",
        "        return self.classifier(y)\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.loss_metric]\n",
        "\n",
        "    def train_step(self, batch):\n",
        "        \"\"\"Processes one batch inside model.fit().\"\"\"\n",
        "        source = batch[\"source\"]\n",
        "        target = batch[\"target\"]\n",
        "        dec_input = target[:, :-1]\n",
        "        dec_target = target[:, 1:]\n",
        "        with tf.GradientTape() as tape:\n",
        "            preds = self([source, dec_input])\n",
        "            one_hot = tf.one_hot(dec_target, depth=self.num_classes)\n",
        "            mask = tf.math.logical_not(tf.math.equal(dec_target, 0))\n",
        "            loss = self.compiled_loss(one_hot, preds, sample_weight=mask)\n",
        "        trainable_vars = self.trainable_variables\n",
        "        gradients = tape.gradient(loss, trainable_vars)\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "        self.loss_metric.update_state(loss)\n",
        "        return {\"loss\": self.loss_metric.result()}\n",
        "\n",
        "    def test_step(self, batch):\n",
        "        source = batch[\"source\"]\n",
        "        target = batch[\"target\"]\n",
        "        dec_input = target[:, :-1]\n",
        "        dec_target = target[:, 1:]\n",
        "        preds = self([source, dec_input])\n",
        "        one_hot = tf.one_hot(dec_target, depth=self.num_classes)\n",
        "        mask = tf.math.logical_not(tf.math.equal(dec_target, 0))\n",
        "        loss = self.compiled_loss(one_hot, preds, sample_weight=mask)\n",
        "        self.loss_metric.update_state(loss)\n",
        "        return {\"loss\": self.loss_metric.result()}\n",
        "\n",
        "    def generate(self, source, target_start_token_idx):\n",
        "        \"\"\"Performs inference over one batch of inputs using greedy decoding.\"\"\"\n",
        "        bs = tf.shape(source)[0]\n",
        "        enc = self.encoder(source)\n",
        "        dec_input = tf.ones((bs, 1), dtype=tf.int32) * target_start_token_idx\n",
        "        dec_logits = []\n",
        "        for i in range(self.target_maxlen - 1):\n",
        "            dec_out = self.decode(enc, dec_input)\n",
        "            logits = self.classifier(dec_out)\n",
        "            logits = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
        "            last_logit = tf.expand_dims(logits[:, -1], axis=-1)\n",
        "            dec_logits.append(last_logit)\n",
        "            dec_input = tf.concat([dec_input, last_logit], axis=-1)\n",
        "        return dec_input"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mBVv4038co1"
      },
      "source": [
        "### Extract"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSDFAddr8Ad0"
      },
      "source": [
        "class TrainTestDataset:\n",
        "  def organize_file(self, origin, new_path, newer_path, zip_file):\n",
        "    get_clean_file = GetCleanFile(origin,new_path,newer_path, zip_file)\n",
        "    dir_names = get_clean_file().get_path().get_subdirectory()\n",
        "    get_clean_file.rename_and_move_dir(dir_names)\n",
        "    get_clean_file.delete_directory()\n",
        "\n",
        "    subdir = tf.io.gfile.glob(newer_path + '/*/*.txt')\n",
        "    subdir_1 = tf.io.gfile.glob(newer_path + '/*/*.flac')\n",
        "    get_clean_file.clean_label(subdir)\n",
        "    return self\n",
        "\n",
        "  def flac_conversion(self, path):\n",
        "    file_conversion = AudioFileConversion()\n",
        "    file_conversion.file_conversion(path)\n",
        "    return self\n",
        "\n",
        "  def prepare_dataset(self, path):\n",
        "    AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "    audio_file = sorted(np.array(tf.io.gfile.glob(str(newer_path) + '/*/*.wav')))\n",
        "    text_file = sorted(np.array(tf.io.gfile.glob(str(newer_path) + '/*/*.txt')))\n",
        "    list_ds = tf.data.Dataset.from_tensor_slices((audio_file, text_file))\n",
        "    return list_ds\n",
        "\n",
        "  def preprocess_dataset(self, list_ds, train):\n",
        "    AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "    get_waveform_label = GetWaveformLabel()\n",
        "    get_process_data_label = GetProcessDataLabel()\n",
        "  \n",
        "    waveform_ds = list_ds.map(get_waveform_label.get_waveform_label, \n",
        "                            num_parallel_calls=AUTOTUNE)\n",
        "    waveform_ds = waveform_ds.prefetch(AUTOTUNE)\n",
        "\n",
        "    spectrogram_ds = waveform_ds.map(\n",
        "        lambda x,y : get_process_data_label.get_process_label(x, y, train), \n",
        "        num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "    ds = spectrogram_ds.map(lambda x, y: {\"source\": x, \"target\": y}).cache()\n",
        "    ds = ds.prefetch(AUTOTUNE)\n",
        "\n",
        "    return ds\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_pQH3jPGomF",
        "outputId": "97eb77f7-eaec-4625-a349-c48c079337e0"
      },
      "source": [
        "\n",
        "origin = 'https://www.openslr.org/resources/12/train-clean-100.tar.gz'\n",
        "new_path = './data'\n",
        "newer_path = './DATA'\n",
        "\n",
        "val_origin = 'https://www.openslr.org/resources/12/test-clean.tar.gz'\n",
        "val_new_path = './val_data'\n",
        "val_newer_path = './VAL_DATA'\n",
        "\n",
        "create_train_ds = TrainTestDataset()\n",
        "create_test_ds = TrainTestDataset()\n",
        "\n",
        "create_train_ds.organize_file(origin, new_path, newer_path, 'train.zip')\n",
        "create_train_ds.flac_conversion(newer_path)\n",
        "list_ds = create_train_ds.prepare_dataset(newer_path)\n",
        "train_ds = create_train_ds.preprocess_dataset(list_ds, train=True)\n",
        "\n",
        "create_test_ds.organize_file(val_origin, val_new_path, val_newer_path, 'test.zip')\n",
        "create_test_ds.flac_conversion(val_newer_path)\n",
        "val_list_ds = create_test_ds.prepare_dataset(val_newer_path)\n",
        "val_ds = create_test_ds.preprocess_dataset(val_list_ds, train=False)\n",
        "\n",
        "train_ds = train_ds.shuffle(1024).batch(16)\n",
        "val_ds = val_ds.batch(4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.openslr.org/resources/12/test-clean.tar.gz\n",
            "346669056/346663984 [==============================] - 3s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:31: UserWarning: Creating resources inside a function passed to Dataset.map() is not supported. Create each resource outside the function, and capture it inside the function to use it.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wm3jX9_r4uk7"
      },
      "source": [
        "###TEST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "By5sHmiIJG_2"
      },
      "source": [
        "def create_model():\n",
        "  model = Transformer(\n",
        "      num_hid=200,\n",
        "      num_head=2,\n",
        "      num_feed_forward=400,\n",
        "      target_maxlen=400,\n",
        "      num_layers_enc=4,\n",
        "      num_layers_dec=1,\n",
        "      num_classes=29,\n",
        "    )\n",
        "  loss_fn = tf.keras.losses.CategoricalCrossentropy(\n",
        "      from_logits=True, label_smoothing=0.1,\n",
        "    )\n",
        "\n",
        "  optimizer = keras.optimizers.Adam()\n",
        "  model.compile(optimizer=optimizer, loss=loss_fn)\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2s2oQX0L1yoo"
      },
      "source": [
        "Run cell dibawah buat train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzVDKSv803Hu"
      },
      "source": [
        "checkpoint_path = \"./training/cp.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "# Create a callback that saves the model's weights\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deFqQUGstztc",
        "outputId": "4fce9823-34bf-449e-92a6-a6e56cf229d9"
      },
      "source": [
        "\n",
        "model = create_model()\n",
        "\n",
        "epochs = 1\n",
        "\n",
        "history = model.fit(train_ds,validation_data=val_ds,\n",
        "                    epochs=epochs, verbose=1,\n",
        "                    callbacks=[cp_callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1784/1784 [==============================] - 4684s 3s/step - loss: 1.1138 - val_loss: 0.9981\n",
            "\n",
            "Epoch 00001: saving model to ./training/cp.ckpt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HTHNobi14fv"
      },
      "source": [
        "Kalau model belum selesai ditrain, save dulu file checkpointnya terus run kode dibawah"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KdJoxzUWuVMq",
        "outputId": "2325880d-dc42-4dd2-afc4-0a0cf28b7425"
      },
      "source": [
        "model = create_model()\n",
        "\n",
        "# Load the previously saved weights\n",
        "model.load_weights('./training/cp.ckpt')\n",
        "\n",
        "epochs=1\n",
        "\n",
        "history = model.fit(train_ds,validation_data=val_ds,\n",
        "                    epochs=epochs, verbose=1,\n",
        "                    callbacks=[cp_callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1784/1784 [==============================] - 167s 90ms/step - loss: 0.9657 - val_loss: 0.9515\n",
            "\n",
            "Epoch 00001: saving model to ./training/cp.ckpt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gm-wBFzuvdu",
        "outputId": "44e4a78b-a8ae-4135-ac26-7813d0378092"
      },
      "source": [
        "model.save('saved_model/my_model')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.embeddings.Embedding object at 0x7f78081bcc50>, because it is not built.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.embeddings.Embedding object at 0x7f78081bcc50>, because it is not built.\n",
            "WARNING:absl:Found untraced functions such as conv1d_15_layer_call_and_return_conditional_losses, conv1d_15_layer_call_fn, conv1d_16_layer_call_and_return_conditional_losses, conv1d_16_layer_call_fn, conv1d_17_layer_call_and_return_conditional_losses while saving (showing 5 of 345). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as conv1d_15_layer_call_and_return_conditional_losses, conv1d_15_layer_call_fn, conv1d_16_layer_call_and_return_conditional_losses, conv1d_16_layer_call_fn, conv1d_17_layer_call_and_return_conditional_losses while saving (showing 5 of 345). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: saved_model/my_model/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: saved_model/my_model/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbeCcdMvhcms"
      },
      "source": [
        "audio_data = tf.io.read_file('./DATA/0/445-123857-0000.wav')\n",
        "audio_decoding = EncodingDecoding()\n",
        "waveform = audio_decoding.decode_audio(audio_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_vfhqIl4D6N"
      },
      "source": [
        "spectrogram=get_process_waveform(waveform)\n",
        "spectrogram = tf.expand_dims(spectrogram, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUBo4AJ_mTWI"
      },
      "source": [
        "def inference(model, spectrogram):\n",
        "  label = model.generate(spectrogram,0)\n",
        "\n",
        "  decoding = EncodingDecoding()\n",
        "  label = decoding.decode_label(tf.cast(tf.squeeze(label, axis=0),\n",
        "                                        dtype=tf.int64))\n",
        "  \n",
        "  label = b''.join(label).decode('utf-8')\n",
        "  return label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "OycJ9lmUjH7G",
        "outputId": "23894835-d036-4c82-e9d2-2448282f1be9"
      },
      "source": [
        "inference(model, spectrogram)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'-he was the stood the station the stood the said the startion the stood the station the stand the start the station the stand the stand the stand the stand the stating the the stone ther the sare the stond the the the the the the the the the the the the the the the wan the the was the the the the the ast the the s the the t ont the ont wont wind st t s ast ent on  onthe w the the  s the s  t the o'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTP__7CunOfX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}